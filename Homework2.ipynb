{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5eaca6a-9170-407f-a4c2-4f08a0736b15",
   "metadata": {
    "tags": []
   },
   "source": [
    "# MTH 4320 / 5320 - Homework 2\n",
    "\n",
    "## Dense Neural Networks and Keras\n",
    "\n",
    "**Deadline**: Oct 3\n",
    "\n",
    "**Points**: 50\n",
    "\n",
    "### Instructions\n",
    "\n",
    "Submit **one** Python notebook file for grading. Your file must include **text explanations** of your work, **well-commented code**, and the **outputs** from your code.\n",
    "\n",
    "### Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31a1580",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db562dfc",
   "metadata": {},
   "source": [
    "#### Gradients\n",
    "\n",
    "1. [10 points] Consider a single neuron with 3 inputs and PReLU activation function. Find the mathematical formula for the gradient of the activated output with respect to its incoming weights **and** the learnable PReLU parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f22325",
   "metadata": {},
   "source": [
    "The weighted sum $z$ of a single neuron with 3 inputs can be expressed as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c491493",
   "metadata": {},
   "source": [
    "$$z = w_1 x_1 + w_2 x_2 + w_3 x_3 + b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145b4a54",
   "metadata": {},
   "source": [
    "Where $w$ is the weight of neuron, $x$ is the input of neuron, and $b$ is the bias term. The gradient of the weighted sum is then the gradient with respect to each weight:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57aa2755",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial z}{\\partial w_1} = x_1 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57170908",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial z}{\\partial w_2} = x_2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96ff38f",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial z}{\\partial w_3} = x_3 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dce0a7c",
   "metadata": {},
   "source": [
    "Next, let's find the gradient of the PReLU function. The PReLU function is defined as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7eb55f",
   "metadata": {},
   "source": [
    "$$ f(x) = \\left\\{ \n",
    "    \\begin{array}{ll}\n",
    "    x & x > 0 \\\\\n",
    "    \\alpha x & x \\le 0 \\\\\n",
    "    \\end{array}\n",
    "\\right. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206b97f1",
   "metadata": {},
   "source": [
    "The gradient with respect to the learnable parameter $\\alpha$ is:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a754b3bd",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial f(x)}{\\partial \\alpha} = \\left\\{ \n",
    "    \\begin{array}{ll}\n",
    "    x & \\alpha x > x \\\\\n",
    "    0 & \\alpha x \\le x\\\\\n",
    "    \\end{array}\n",
    "\\right. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28959018",
   "metadata": {},
   "source": [
    "The gradient with respect to the weighted sum $z$ is:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914e895d",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial f(z)}{\\partial z} = \\left\\{ \n",
    "    \\begin{array}{ll}\n",
    "    \\alpha & \\alpha z > z \\\\\n",
    "    1 & \\alpha z \\le z\\\\\n",
    "    \\end{array}\n",
    "\\right. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31815830",
   "metadata": {},
   "source": [
    "We then need to find the gradient with respect to the weights. This can be done with the chain rule:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee857258",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial f(z)}{\\partial w_1} = \\frac{\\partial f(z)}{\\partial z} \\frac{\\partial z}{\\partial w_1} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d98caa",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial f(z)}{\\partial w_2} = \\frac{\\partial f(z)}{\\partial z} \\frac{\\partial z}{\\partial w_2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebe70a4",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial f(z)}{\\partial w_3} = \\frac{\\partial f(z)}{\\partial z} \\frac{\\partial z}{\\partial w_3} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de1819a",
   "metadata": {},
   "source": [
    "We will also need is the gradient with respect to the learnable parameter $\\alpha$, which can once again be done with chain rule:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7562edfb",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial f(z)}{\\partial \\alpha} = \\frac{\\partial f(z)}{\\partial z} \\frac{\\partial z}{\\partial \\alpha} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c72f81",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial z}{\\partial \\alpha} = \\frac{\\partial \\alpha x}{\\partial \\alpha} = x$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6519e1d",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial f(z)}{\\partial \\alpha} = \\frac{\\partial f(z)}{\\partial z} x $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160998dc",
   "metadata": {},
   "source": [
    "Now we can put it all together to get our final formula for the gradient of the activated output with respect to its incoming weights and the learnable PReLU parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e2fc15",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial f(z)}{\\partial w_1} = \\left\\{ \n",
    "    \\begin{array}{ll}\n",
    "    x_1 \\alpha & \\alpha z > z \\\\\n",
    "    x_1 & \\alpha z \\le z \\\\\n",
    "    \\end{array}\n",
    "\\right. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f9bf6b",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial f(z)}{\\partial w_2} = \\left\\{ \n",
    "    \\begin{array}{ll}\n",
    "    x_2 \\alpha & \\alpha z > z \\\\\n",
    "    x_2 & \\alpha z \\le z \\\\\n",
    "    \\end{array}\n",
    "\\right. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436a7cf7",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial f(z)}{\\partial w_3} = \\left\\{ \n",
    "    \\begin{array}{ll}\n",
    "    x_3 \\alpha & \\alpha z > z \\\\\n",
    "    _3 & \\alpha z \\le z \\\\\n",
    "    \\end{array}\n",
    "\\right. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b36b2e",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial f(z)}{\\partial \\alpha} = \\left\\{ \n",
    "    \\begin{array}{ll}\n",
    "    x z & \\alpha z > z \\\\\n",
    "    0 & \\alpha z \\le z \\\\\n",
    "    \\end{array}\n",
    "\\right. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6400f31c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ee2fb0",
   "metadata": {},
   "source": [
    "#### Dense Neural Networks\n",
    "\n",
    "2. [40 points]. Use a feedforward NN with SGD to classify the CIFAR-10 dataset, and tune its hyperparameters as best you can. **You must use Keras or PyTorch**. Requirements below. \n",
    "\n",
    "Randomly split the dataset into 60\\%/20/\\%/20\\% training/validation/testing sets. When tuning hyperparameters, test on the validation set. After you find the best hyperparameters, run your code **once** with these settings on the test. Use `random_state = 1` before splitting data.\n",
    "\n",
    "Start with a 1-node classifier as a benchmark.\n",
    "\n",
    "You must run **at least one experiment** using all major techniques (5 points each):\n",
    "\n",
    "* Normalization/Standardization\n",
    "* Weight Initialization\n",
    "* Architectures\n",
    "* Activation functions\n",
    "* Loss functions\n",
    "* Regularization (must include dropout)\n",
    "\n",
    "**For each experiment, document why you chose to run this experiment, training accuracy/loss, validation accuracy/loss, epoch number with best validation accuracy (see the `EarlyStopping` callback), and training runtime.**\n",
    "\n",
    "Training takes significant time, so brute force is *not* feasible. Make *informed decisions* on how to proceed and write your reasoning in your report. Include all fruitful experiments you run along the way. More importantly than the results, I want to see that you are *thinking well* and making good decisions. Good results will come from eventually if you *understand what you are doing*.\n",
    "\n",
    "**Explanations and reasoning for your progression = [10 points]**\n",
    "\n",
    "**Recommended:** Use small training sets for your initial tests so it works more quickly and then scale up when you results get better.\n",
    "\n",
    "**Bonus:** Top 3 highest classification accuracy submissions earn +5 points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "edc336f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overfitting => Use regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66af4445",
   "metadata": {},
   "source": [
    "Let's begin by importing all the necessary libraries from tensorflow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "27d47a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.layers import PReLU \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfd41ed",
   "metadata": {},
   "source": [
    "Let's also define some parameters we will be using throughout out code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "5cee4278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "learning_rate = 0.0001\n",
    "epochs = 20\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd97bb7c",
   "metadata": {},
   "source": [
    "The dataset itself can be directly loaded from keras. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "9a8ec0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CIFAR-10 dataset\n",
    "(trainX, trainY), (testX, testY) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f05424",
   "metadata": {},
   "source": [
    "The data is then split into 60% training, 20% validation, and 20% testing, all using `random_state = 1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "8165316d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (30000, 32, 32, 3)\n",
      "Validation data shape: (10000, 32, 32, 3)\n",
      "Testing data shape: (10000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training, validation, and testing sets\n",
    "trainX, tempX, trainY, tempY = train_test_split(trainX, trainY, test_size=0.4, random_state=1)\n",
    "validX, testX, validY, testY = train_test_split(tempX, tempY, test_size=0.5, random_state=1)\n",
    "\n",
    "# Check the shapes of the resulting sets\n",
    "print(\"Training data shape:\", trainX.shape)\n",
    "print(\"Validation data shape:\", validX.shape)\n",
    "print(\"Testing data shape:\", testX.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1cac4a",
   "metadata": {},
   "source": [
    "Next, we want to ensure the labels are properly setup with one-hot enconding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "ac5eb563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to one-hot encoding\n",
    "num_classes = 10\n",
    "trainY = tf.keras.utils.to_categorical(trainY, num_classes)\n",
    "testY = tf.keras.utils.to_categorical(testY, num_classes)\n",
    "validY = tf.keras.utils.to_categorical(validY, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63bd626",
   "metadata": {},
   "source": [
    "Now we can set up a 1-node classifier as a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "c43c1237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a 1-node classifier\n",
    "baselineModel = Sequential([\n",
    "    Flatten(input_shape=(32,32,3)),\n",
    "    Dense(1, activation='relu'),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "]) \n",
    "\n",
    "# Compile the model\n",
    "baselineModel.compile(loss='categorical_crossentropy', optimizer=SGD(learning_rate=learning_rate), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509445f6",
   "metadata": {},
   "source": [
    "We want to also define early stopping in case our model sees no improvement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "c0dac4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the early stopping callback\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389701d3",
   "metadata": {},
   "source": [
    "Let's try running the model with no optimizations to get our baseline results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "7eea4587",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "469/469 [==============================] - 3s 4ms/step - loss: 2.3063 - accuracy: 0.0968 - val_loss: 2.3026 - val_accuracy: 0.0956\n",
      "Epoch 2/20\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 2.3026 - accuracy: 0.1013 - val_loss: 2.3026 - val_accuracy: 0.0956\n",
      "Epoch 3/20\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 2.3026 - accuracy: 0.1020 - val_loss: 2.3026 - val_accuracy: 0.0956\n",
      "Epoch 4/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 2.3026 - accuracy: 0.1020 - val_loss: 2.3026 - val_accuracy: 0.0956\n",
      "313/313 [==============================] - 1s 2ms/step\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      1017\n",
      "           1       0.00      0.00      0.00       990\n",
      "           2       0.00      0.00      0.00      1061\n",
      "           3       0.00      0.00      0.00       994\n",
      "           4       0.00      0.00      0.00       936\n",
      "           5       0.00      0.00      0.00      1009\n",
      "           6       0.00      0.00      0.00      1034\n",
      "           7       0.00      0.00      0.00      1005\n",
      "           8       0.10      1.00      0.17       956\n",
      "           9       0.00      0.00      0.00       998\n",
      "\n",
      "    accuracy                           0.10     10000\n",
      "   macro avg       0.01      0.10      0.02     10000\n",
      "weighted avg       0.01      0.10      0.02     10000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/deep_learning/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/envs/deep_learning/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/envs/deep_learning/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Train the model on the training data\n",
    "baselineModel.fit(trainX, trainY, epochs=epochs, batch_size=batch_size, validation_data=(validX, validY), callbacks=[early_stopping])\n",
    "\n",
    "# Use the trained model to make predictions on the valid set\n",
    "valid_predictions = baselineModel.predict(validX)\n",
    "\n",
    "# Convert predictions to class labels\n",
    "valid_predictions_labels = np.argmax(valid_predictions, axis=1)\n",
    "valid_true_labels = np.argmax(validY, axis=1)\n",
    "\n",
    "# Generate a classification report\n",
    "classification_rep = classification_report(valid_true_labels, valid_predictions_labels)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019975f0",
   "metadata": {},
   "source": [
    "As we can see here, the baseline compiles, but there is not much improvement to be seen, and the accuracy ends up around 10%. This shows the necesity to have a more complex and layered neural network. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9459b79",
   "metadata": {},
   "source": [
    "For this purpose, we'll construct a feedforward neural network, in which we'll be using ReLU and softmax layers with categorical crossentropy loss and the SGD optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "06766041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a feedforward neural net\n",
    "model = Sequential()\n",
    "\n",
    "# Create the layers\n",
    "model.add(Flatten(input_shape=(32,32,3)))\n",
    "model.add(Dense(256, activation = 'relu'))\n",
    "model.add(Dense(128, activation = 'relu'))\n",
    "model.add(Dense(10, activation = 'softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=SGD(learning_rate=learning_rate), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3d3e3c",
   "metadata": {},
   "source": [
    "Let's see what happens when we run this model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "6f0929a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "469/469 [==============================] - 5s 9ms/step - loss: 8.6833 - accuracy: 0.1024 - val_loss: 2.5672 - val_accuracy: 0.1080\n",
      "Epoch 2/20\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 2.4443 - accuracy: 0.1051 - val_loss: 2.4506 - val_accuracy: 0.1026\n",
      "Epoch 3/20\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 2.3789 - accuracy: 0.1073 - val_loss: 2.4210 - val_accuracy: 0.1001\n",
      "Epoch 4/20\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 2.3497 - accuracy: 0.1118 - val_loss: 2.4022 - val_accuracy: 0.0997\n",
      "Epoch 5/20\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 2.3314 - accuracy: 0.1146 - val_loss: 2.3915 - val_accuracy: 0.1028\n",
      "Epoch 6/20\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 2.3178 - accuracy: 0.1167 - val_loss: 2.3846 - val_accuracy: 0.1044\n",
      "Epoch 7/20\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 2.3076 - accuracy: 0.1206 - val_loss: 2.3787 - val_accuracy: 0.1058\n",
      "Epoch 8/20\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 2.2994 - accuracy: 0.1216 - val_loss: 2.3728 - val_accuracy: 0.1133\n",
      "Epoch 9/20\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 2.2914 - accuracy: 0.1266 - val_loss: 2.3700 - val_accuracy: 0.1079\n",
      "Epoch 10/20\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 2.2817 - accuracy: 0.1300 - val_loss: 2.3640 - val_accuracy: 0.1182\n",
      "Epoch 11/20\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 2.2726 - accuracy: 0.1366 - val_loss: 2.3565 - val_accuracy: 0.1246\n",
      "Epoch 12/20\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 2.2607 - accuracy: 0.1432 - val_loss: 2.3448 - val_accuracy: 0.1359\n",
      "Epoch 13/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 2.2470 - accuracy: 0.1498 - val_loss: 2.3291 - val_accuracy: 0.1417\n",
      "Epoch 14/20\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 2.2305 - accuracy: 0.1565 - val_loss: 2.3114 - val_accuracy: 0.1477\n",
      "Epoch 15/20\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 2.2106 - accuracy: 0.1638 - val_loss: 2.2925 - val_accuracy: 0.1549\n",
      "Epoch 16/20\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 2.1912 - accuracy: 0.1704 - val_loss: 2.2736 - val_accuracy: 0.1560\n",
      "Epoch 17/20\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 2.1758 - accuracy: 0.1713 - val_loss: 2.2618 - val_accuracy: 0.1610\n",
      "Epoch 18/20\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 2.1677 - accuracy: 0.1773 - val_loss: 2.2575 - val_accuracy: 0.1615\n",
      "Epoch 19/20\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 2.1627 - accuracy: 0.1780 - val_loss: 2.2532 - val_accuracy: 0.1625\n",
      "Epoch 20/20\n",
      "469/469 [==============================] - 4s 10ms/step - loss: 2.1584 - accuracy: 0.1762 - val_loss: 2.2498 - val_accuracy: 0.1707\n",
      "313/313 [==============================] - 1s 4ms/step\n",
      "Classification Report for Default Model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.10      0.01      0.01      1017\n",
      "           1       0.13      0.01      0.01       990\n",
      "           2       0.17      0.00      0.01      1061\n",
      "           3       0.20      0.06      0.10       994\n",
      "           4       0.07      0.00      0.00       936\n",
      "           5       0.27      0.03      0.05      1009\n",
      "           6       0.15      0.81      0.26      1034\n",
      "           7       0.16      0.01      0.01      1005\n",
      "           8       0.18      0.56      0.27       956\n",
      "           9       0.26      0.21      0.23       998\n",
      "\n",
      "    accuracy                           0.17     10000\n",
      "   macro avg       0.17      0.17      0.10     10000\n",
      "weighted avg       0.17      0.17      0.10     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model on the training data\n",
    "model.fit(trainX, trainY, epochs=epochs, batch_size=batch_size, validation_data=(validX, validY), callbacks=[early_stopping])\n",
    "\n",
    "# Use the trained model to make predictions on the valid set\n",
    "valid_predictions = model.predict(validX)\n",
    "\n",
    "# Convert predictions to class labels\n",
    "valid_predictions_labels = np.argmax(valid_predictions, axis=1)\n",
    "valid_true_labels = np.argmax(validY, axis=1)\n",
    "\n",
    "# Generate a classification report\n",
    "classification_rep = classification_report(valid_true_labels, valid_predictions_labels)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification Report for Default Model:\")\n",
    "print(classification_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8a7c38",
   "metadata": {},
   "source": [
    "Here we get some better results than our baseline, and some of the numbers are pretty well predicted. With this in mind, we can now start experimenting with hyperparamters and tuning towards a final model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea52d1b0",
   "metadata": {},
   "source": [
    "##### Normalization/Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad96663",
   "metadata": {},
   "source": [
    "Let's try normalizing our data, and see if that yields similar results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "5d87a7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a feedforward neural net\n",
    "model = Sequential()\n",
    "\n",
    "# Create the layers\n",
    "model.add(Flatten(input_shape=(32,32,3)))\n",
    "model.add(Dense(256, activation = 'relu'))\n",
    "model.add(Dense(128, activation = 'relu'))\n",
    "model.add(Dense(10, activation = 'softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=SGD(learning_rate=learning_rate), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "fe13463e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the pixel values to the range [0, 1]\n",
    "trainX = trainX.astype('float32') / 255.0\n",
    "testX = testX.astype('float32') / 255.0\n",
    "validX = validX.astype('float32') / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "489b6f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "469/469 [==============================] - 8s 14ms/step - loss: 2.3195 - accuracy: 0.1220 - val_loss: 2.2875 - val_accuracy: 0.1342\n",
      "Epoch 2/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 2.2698 - accuracy: 0.1464 - val_loss: 2.2603 - val_accuracy: 0.1555\n",
      "Epoch 3/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 2.2467 - accuracy: 0.1681 - val_loss: 2.2403 - val_accuracy: 0.1765\n",
      "Epoch 4/20\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 2.2275 - accuracy: 0.1866 - val_loss: 2.2223 - val_accuracy: 0.1896\n",
      "Epoch 5/20\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 2.2101 - accuracy: 0.1994 - val_loss: 2.2061 - val_accuracy: 0.2093\n",
      "Epoch 6/20\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 2.1942 - accuracy: 0.2127 - val_loss: 2.1908 - val_accuracy: 0.2191\n",
      "Epoch 7/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 2.1793 - accuracy: 0.2207 - val_loss: 2.1767 - val_accuracy: 0.2247\n",
      "Epoch 8/20\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 2.1657 - accuracy: 0.2291 - val_loss: 2.1639 - val_accuracy: 0.2322\n",
      "Epoch 9/20\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 2.1531 - accuracy: 0.2357 - val_loss: 2.1520 - val_accuracy: 0.2383\n",
      "Epoch 10/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 2.1411 - accuracy: 0.2417 - val_loss: 2.1404 - val_accuracy: 0.2465\n",
      "Epoch 11/20\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 2.1298 - accuracy: 0.2474 - val_loss: 2.1295 - val_accuracy: 0.2524\n",
      "Epoch 12/20\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 2.1192 - accuracy: 0.2523 - val_loss: 2.1193 - val_accuracy: 0.2556\n",
      "Epoch 13/20\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 2.1092 - accuracy: 0.2561 - val_loss: 2.1093 - val_accuracy: 0.2596\n",
      "Epoch 14/20\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 2.0996 - accuracy: 0.2600 - val_loss: 2.1000 - val_accuracy: 0.2642\n",
      "Epoch 15/20\n",
      "469/469 [==============================] - 4s 10ms/step - loss: 2.0905 - accuracy: 0.2632 - val_loss: 2.0914 - val_accuracy: 0.2660\n",
      "Epoch 16/20\n",
      "469/469 [==============================] - 4s 7ms/step - loss: 2.0819 - accuracy: 0.2665 - val_loss: 2.0825 - val_accuracy: 0.2713\n",
      "Epoch 17/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 2.0736 - accuracy: 0.2694 - val_loss: 2.0744 - val_accuracy: 0.2724\n",
      "Epoch 18/20\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 2.0657 - accuracy: 0.2720 - val_loss: 2.0668 - val_accuracy: 0.2770\n",
      "Epoch 19/20\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 2.0581 - accuracy: 0.2761 - val_loss: 2.0592 - val_accuracy: 0.2798\n",
      "Epoch 20/20\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 2.0507 - accuracy: 0.2784 - val_loss: 2.0516 - val_accuracy: 0.2824\n",
      "313/313 [==============================] - 1s 3ms/step\n",
      "Classification Report for Normalized Model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.48      0.39      1017\n",
      "           1       0.43      0.12      0.18       990\n",
      "           2       0.22      0.07      0.11      1061\n",
      "           3       0.18      0.12      0.14       994\n",
      "           4       0.24      0.31      0.27       936\n",
      "           5       0.23      0.39      0.29      1009\n",
      "           6       0.28      0.20      0.24      1034\n",
      "           7       0.27      0.15      0.19      1005\n",
      "           8       0.33      0.50      0.40       956\n",
      "           9       0.31      0.52      0.39       998\n",
      "\n",
      "    accuracy                           0.28     10000\n",
      "   macro avg       0.28      0.28      0.26     10000\n",
      "weighted avg       0.28      0.28      0.26     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model on the training data\n",
    "model.fit(trainX, trainY, epochs=epochs, batch_size=batch_size, validation_data=(validX, validY), callbacks=[early_stopping])\n",
    "\n",
    "# Use the trained model to make predictions on the valid set\n",
    "valid_predictions = model.predict(validX)\n",
    "\n",
    "# Convert predictions to class labels\n",
    "valid_predictions_labels = np.argmax(valid_predictions, axis=1)\n",
    "valid_true_labels = np.argmax(validY, axis=1)\n",
    "\n",
    "# Generate a classification report\n",
    "classification_rep = classification_report(valid_true_labels, valid_predictions_labels)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification Report for Normalized Model:\")\n",
    "print(classification_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506a313c",
   "metadata": {},
   "source": [
    "Here the results have gotten much better. However, we notice that the learning rate is still increasing throughout the epochs and there may be room to grow. We could try adding more epochs, but seeing how slow the loss is changing, let's instead try changing the learning rate to something a bit higher. It is important to note here that we are seeing the training loss and accuaracy being larger than the valid loss and accuracy. This is a sign of overfitting and needs to be addressed, however, with a new learning rate this may change so lets first experiment with that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "87ccf0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change learning rate\n",
    "learning_rate = 0.001 \n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=SGD(learning_rate=learning_rate), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "ecfa4c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "469/469 [==============================] - 6s 11ms/step - loss: 2.0181 - accuracy: 0.2880 - val_loss: 1.9921 - val_accuracy: 0.3029\n",
      "Epoch 2/20\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 1.9673 - accuracy: 0.3065 - val_loss: 1.9528 - val_accuracy: 0.3163\n",
      "Epoch 3/20\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 1.9319 - accuracy: 0.3212 - val_loss: 1.9183 - val_accuracy: 0.3281\n",
      "Epoch 4/20\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 1.9048 - accuracy: 0.3301 - val_loss: 1.8954 - val_accuracy: 0.3354\n",
      "Epoch 5/20\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 1.8826 - accuracy: 0.3424 - val_loss: 1.8742 - val_accuracy: 0.3460\n",
      "Epoch 6/20\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 1.8647 - accuracy: 0.3478 - val_loss: 1.8567 - val_accuracy: 0.3510\n",
      "Epoch 7/20\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 1.8484 - accuracy: 0.3548 - val_loss: 1.8433 - val_accuracy: 0.3579\n",
      "Epoch 8/20\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 1.8344 - accuracy: 0.3606 - val_loss: 1.8283 - val_accuracy: 0.3623\n",
      "Epoch 9/20\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 1.8216 - accuracy: 0.3651 - val_loss: 1.8169 - val_accuracy: 0.3653\n",
      "Epoch 10/20\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 1.8098 - accuracy: 0.3690 - val_loss: 1.8066 - val_accuracy: 0.3724\n",
      "Epoch 11/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 1.7992 - accuracy: 0.3745 - val_loss: 1.7976 - val_accuracy: 0.3757\n",
      "Epoch 12/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 1.7886 - accuracy: 0.3771 - val_loss: 1.7903 - val_accuracy: 0.3806\n",
      "Epoch 13/20\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 1.7793 - accuracy: 0.3798 - val_loss: 1.7790 - val_accuracy: 0.3831\n",
      "Epoch 14/20\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 1.7698 - accuracy: 0.3840 - val_loss: 1.7697 - val_accuracy: 0.3841\n",
      "Epoch 15/20\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 1.7618 - accuracy: 0.3883 - val_loss: 1.7626 - val_accuracy: 0.3843\n",
      "Epoch 16/20\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 1.7533 - accuracy: 0.3906 - val_loss: 1.7571 - val_accuracy: 0.3866\n",
      "Epoch 17/20\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 1.7455 - accuracy: 0.3937 - val_loss: 1.7499 - val_accuracy: 0.3904\n",
      "Epoch 18/20\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 1.7377 - accuracy: 0.3959 - val_loss: 1.7455 - val_accuracy: 0.3914\n",
      "Epoch 19/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 1.7307 - accuracy: 0.3991 - val_loss: 1.7388 - val_accuracy: 0.3931\n",
      "Epoch 20/20\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 1.7237 - accuracy: 0.4022 - val_loss: 1.7289 - val_accuracy: 0.3983\n",
      "313/313 [==============================] - 1s 3ms/step\n",
      "Classification Report for Normalized Model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.51      0.45      1017\n",
      "           1       0.44      0.45      0.44       990\n",
      "           2       0.33      0.25      0.29      1061\n",
      "           3       0.30      0.25      0.27       994\n",
      "           4       0.34      0.33      0.33       936\n",
      "           5       0.36      0.36      0.36      1009\n",
      "           6       0.39      0.41      0.40      1034\n",
      "           7       0.45      0.42      0.43      1005\n",
      "           8       0.46      0.56      0.50       956\n",
      "           9       0.47      0.45      0.46       998\n",
      "\n",
      "    accuracy                           0.40     10000\n",
      "   macro avg       0.39      0.40      0.39     10000\n",
      "weighted avg       0.39      0.40      0.39     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model on the training data\n",
    "model.fit(trainX, trainY, epochs=epochs, batch_size=batch_size, validation_data=(validX, validY), callbacks=[early_stopping])\n",
    "\n",
    "# Use the trained model to make predictions on the valid set\n",
    "valid_predictions = model.predict(validX)\n",
    "\n",
    "# Convert predictions to class labels\n",
    "valid_predictions_labels = np.argmax(valid_predictions, axis=1)\n",
    "valid_true_labels = np.argmax(validY, axis=1)\n",
    "\n",
    "# Generate a classification report\n",
    "classification_rep = classification_report(valid_true_labels, valid_predictions_labels)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification Report for Normalized Model:\")\n",
    "print(classification_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361e20f7",
   "metadata": {},
   "source": [
    "Beautiful! Here we see a hige spike in accuracy thanks to our increased learning rate. However, we still see the training loss and accuracy being better than our valid loss and accuracy, once again displaying overfitting. To fix this overfitting, the best course of action is to implement regularization. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcebb20",
   "metadata": {},
   "source": [
    "##### Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abf4bb5",
   "metadata": {},
   "source": [
    "Let's try regularization using $L^1$ and $L^2$ kernal regularizers. We'll also include a droput rate of 0.5 after the dense layer of 256 units, and a dropout rate of 0.3 after the dense latyer of 128 units. This wil help prevent overfitting by randomly dropping out 50% and 30% of the neurons respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "d8db1b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a feedforward neural net with regularization\n",
    "regularModel = Sequential()\n",
    "\n",
    "# Create the layers\n",
    "regularModel.add(Flatten(input_shape=(32,32,3)))\n",
    "regularModel.add(Dense(256, activation = 'relu', kernel_regularizer = l1_l2(l1 = 0.0, l2 = 0.0001)))\n",
    "regularModel.add(Dropout(0.5))\n",
    "regularModel.add(Dense(128, activation = 'relu', kernel_regularizer = l1_l2(l1 = 0.0, l2 = 0.0001)))\n",
    "regularModel.add(Dropout(0.3))\n",
    "regularModel.add(Dense(10, activation = 'softmax', kernel_regularizer = l1_l2(l1 = 0.0, l2 = 0.0001)))\n",
    "\n",
    "# Compile the model\n",
    "regularModel.compile(loss='categorical_crossentropy', optimizer=SGD(learning_rate=learning_rate), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "821c4ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "469/469 [==============================] - 8s 14ms/step - loss: 2.3926 - accuracy: 0.1354 - val_loss: 2.2841 - val_accuracy: 0.2270\n",
      "Epoch 2/20\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 2.2982 - accuracy: 0.1772 - val_loss: 2.2248 - val_accuracy: 0.2521\n",
      "Epoch 3/20\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 2.2570 - accuracy: 0.1952 - val_loss: 2.1803 - val_accuracy: 0.2662\n",
      "Epoch 4/20\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 2.2264 - accuracy: 0.2067 - val_loss: 2.1464 - val_accuracy: 0.2780\n",
      "Epoch 5/20\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 2.1985 - accuracy: 0.2146 - val_loss: 2.1162 - val_accuracy: 0.2976\n",
      "Epoch 6/20\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 2.1725 - accuracy: 0.2279 - val_loss: 2.0865 - val_accuracy: 0.3035\n",
      "Epoch 7/20\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 2.1515 - accuracy: 0.2364 - val_loss: 2.0626 - val_accuracy: 0.3075\n",
      "Epoch 8/20\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 2.1302 - accuracy: 0.2441 - val_loss: 2.0435 - val_accuracy: 0.3216\n",
      "Epoch 9/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 2.1152 - accuracy: 0.2486 - val_loss: 2.0261 - val_accuracy: 0.3275\n",
      "Epoch 10/20\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 2.0990 - accuracy: 0.2577 - val_loss: 2.0098 - val_accuracy: 0.3330\n",
      "Epoch 11/20\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 2.0851 - accuracy: 0.2625 - val_loss: 1.9930 - val_accuracy: 0.3401\n",
      "Epoch 12/20\n",
      "469/469 [==============================] - 5s 12ms/step - loss: 2.0717 - accuracy: 0.2671 - val_loss: 1.9825 - val_accuracy: 0.3450\n",
      "Epoch 13/20\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 2.0573 - accuracy: 0.2759 - val_loss: 1.9703 - val_accuracy: 0.3483\n",
      "Epoch 14/20\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 2.0498 - accuracy: 0.2829 - val_loss: 1.9578 - val_accuracy: 0.3533\n",
      "Epoch 15/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 2.0407 - accuracy: 0.2795 - val_loss: 1.9499 - val_accuracy: 0.3567\n",
      "Epoch 16/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 2.0319 - accuracy: 0.2876 - val_loss: 1.9407 - val_accuracy: 0.3575\n",
      "Epoch 17/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 2.0199 - accuracy: 0.2884 - val_loss: 1.9308 - val_accuracy: 0.3628\n",
      "Epoch 18/20\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 2.0163 - accuracy: 0.2923 - val_loss: 1.9198 - val_accuracy: 0.3640\n",
      "Epoch 19/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 2.0047 - accuracy: 0.2981 - val_loss: 1.9119 - val_accuracy: 0.3684\n",
      "Epoch 20/20\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 2.0011 - accuracy: 0.3013 - val_loss: 1.9088 - val_accuracy: 0.3689\n",
      "313/313 [==============================] - 1s 3ms/step\n",
      "Classification Report for Regularization:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.41      0.43      1017\n",
      "           1       0.41      0.38      0.39       990\n",
      "           2       0.33      0.13      0.18      1061\n",
      "           3       0.31      0.11      0.16       994\n",
      "           4       0.32      0.29      0.30       936\n",
      "           5       0.36      0.38      0.37      1009\n",
      "           6       0.32      0.51      0.39      1034\n",
      "           7       0.34      0.40      0.36      1005\n",
      "           8       0.41      0.61      0.49       956\n",
      "           9       0.41      0.49      0.45       998\n",
      "\n",
      "    accuracy                           0.37     10000\n",
      "   macro avg       0.36      0.37      0.35     10000\n",
      "weighted avg       0.36      0.37      0.35     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model on the training data\n",
    "regularModel.fit(trainX, trainY, epochs=epochs, batch_size=batch_size, validation_data=(validX, validY), callbacks=[early_stopping])\n",
    "\n",
    "# Use the trained model to make predictions on the valid set\n",
    "valid_predictions = regularModel.predict(validX)\n",
    "\n",
    "# Convert predictions to class labels\n",
    "valid_predictions_labels = np.argmax(valid_predictions, axis=1)\n",
    "valid_true_labels = np.argmax(validY, axis=1)\n",
    "\n",
    "# Generate a classification report\n",
    "classification_rep = classification_report(valid_true_labels, valid_predictions_labels)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification Report for Regularization:\")\n",
    "print(classification_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8c4f96",
   "metadata": {},
   "source": [
    "Here we've finally fixed our problem of overfitting, at the cost of some accuracy. This is an overall a step in the right directions, and we can now try experimenting with other parameters to further increase our accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb4eb8a",
   "metadata": {},
   "source": [
    "##### Weight Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e6f357",
   "metadata": {},
   "source": [
    "Let's try experimenting with weight initialization. For this purpose, we're gonna use the `kernal_initializer` parameters in our dense layers. We're gonna apply some of the most common initializations, which are `glorot_uniform`, `he_normal`, and `lecun_normal`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "63dc0dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 1.3519 - accuracy: 0.5338 - val_loss: 1.4768 - val_accuracy: 0.4788\n",
      "Epoch 2/20\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 1.3496 - accuracy: 0.5363 - val_loss: 1.4733 - val_accuracy: 0.4823\n",
      "Epoch 3/20\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 1.3474 - accuracy: 0.5397 - val_loss: 1.4780 - val_accuracy: 0.4805\n",
      "Epoch 4/20\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 1.3437 - accuracy: 0.5395 - val_loss: 1.4684 - val_accuracy: 0.4823\n",
      "Epoch 5/20\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 1.3422 - accuracy: 0.5390 - val_loss: 1.4766 - val_accuracy: 0.4789\n",
      "Epoch 6/20\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 1.3402 - accuracy: 0.5385 - val_loss: 1.4718 - val_accuracy: 0.4807\n",
      "Epoch 7/20\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 1.3378 - accuracy: 0.5410 - val_loss: 1.4715 - val_accuracy: 0.4821\n",
      "313/313 [==============================] - 1s 3ms/step\n",
      "Classification Report for glorot_uniform:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      1017\n",
      "           1       0.10      0.86      0.18       990\n",
      "           2       0.00      0.00      0.00      1061\n",
      "           3       0.00      0.00      0.00       994\n",
      "           4       0.00      0.00      0.00       936\n",
      "           5       0.25      0.00      0.00      1009\n",
      "           6       0.00      0.00      0.00      1034\n",
      "           7       0.11      0.18      0.14      1005\n",
      "           8       0.00      0.00      0.00       956\n",
      "           9       0.05      0.00      0.00       998\n",
      "\n",
      "    accuracy                           0.10     10000\n",
      "   macro avg       0.05      0.10      0.03     10000\n",
      "weighted avg       0.05      0.10      0.03     10000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/deep_learning/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/envs/deep_learning/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/envs/deep_learning/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 1.3345 - accuracy: 0.5422 - val_loss: 1.4669 - val_accuracy: 0.4845\n",
      "Epoch 2/20\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 1.3340 - accuracy: 0.5396 - val_loss: 1.4726 - val_accuracy: 0.4801\n",
      "Epoch 3/20\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 1.3314 - accuracy: 0.5432 - val_loss: 1.4672 - val_accuracy: 0.4828\n",
      "Epoch 4/20\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 1.3289 - accuracy: 0.5427 - val_loss: 1.4643 - val_accuracy: 0.4832\n",
      "Epoch 5/20\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 1.3256 - accuracy: 0.5427 - val_loss: 1.4715 - val_accuracy: 0.4847\n",
      "Epoch 6/20\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 1.3243 - accuracy: 0.5448 - val_loss: 1.4631 - val_accuracy: 0.4839\n",
      "Epoch 7/20\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 1.3228 - accuracy: 0.5458 - val_loss: 1.4674 - val_accuracy: 0.4840\n",
      "Epoch 8/20\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 1.3191 - accuracy: 0.5457 - val_loss: 1.4602 - val_accuracy: 0.4860\n",
      "Epoch 9/20\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 1.3170 - accuracy: 0.5463 - val_loss: 1.4587 - val_accuracy: 0.4842\n",
      "Epoch 10/20\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 1.3146 - accuracy: 0.5485 - val_loss: 1.4583 - val_accuracy: 0.4879\n",
      "Epoch 11/20\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 1.3124 - accuracy: 0.5477 - val_loss: 1.4550 - val_accuracy: 0.4867\n",
      "Epoch 12/20\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 1.3108 - accuracy: 0.5487 - val_loss: 1.4520 - val_accuracy: 0.4889\n",
      "Epoch 13/20\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 1.3084 - accuracy: 0.5512 - val_loss: 1.4635 - val_accuracy: 0.4839\n",
      "Epoch 14/20\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 1.3056 - accuracy: 0.5523 - val_loss: 1.4552 - val_accuracy: 0.4870\n",
      "Epoch 15/20\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 1.3036 - accuracy: 0.5523 - val_loss: 1.4588 - val_accuracy: 0.4831\n",
      "313/313 [==============================] - 1s 3ms/step\n",
      "Classification Report for he_normal:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.07      0.05      0.06      1017\n",
      "           1       0.08      0.54      0.14       990\n",
      "           2       0.00      0.00      0.00      1061\n",
      "           3       0.14      0.02      0.04       994\n",
      "           4       0.05      0.04      0.04       936\n",
      "           5       0.06      0.00      0.00      1009\n",
      "           6       0.06      0.00      0.01      1034\n",
      "           7       0.00      0.00      0.00      1005\n",
      "           8       0.11      0.13      0.12       956\n",
      "           9       0.14      0.07      0.09       998\n",
      "\n",
      "    accuracy                           0.09     10000\n",
      "   macro avg       0.07      0.09      0.05     10000\n",
      "weighted avg       0.07      0.09      0.05     10000\n",
      "\n",
      "Epoch 1/20\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 1.3008 - accuracy: 0.5544 - val_loss: 1.4518 - val_accuracy: 0.4896\n",
      "Epoch 2/20\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 1.2999 - accuracy: 0.5527 - val_loss: 1.4485 - val_accuracy: 0.4894\n",
      "Epoch 3/20\n",
      "469/469 [==============================] - 4s 10ms/step - loss: 1.2965 - accuracy: 0.5539 - val_loss: 1.4553 - val_accuracy: 0.4838\n",
      "Epoch 4/20\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 1.2954 - accuracy: 0.5526 - val_loss: 1.4461 - val_accuracy: 0.4892\n",
      "Epoch 5/20\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 1.2931 - accuracy: 0.5561 - val_loss: 1.4509 - val_accuracy: 0.4885\n",
      "Epoch 6/20\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 1.2902 - accuracy: 0.5568 - val_loss: 1.4456 - val_accuracy: 0.4901\n",
      "Epoch 7/20\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 1.2879 - accuracy: 0.5562 - val_loss: 1.4544 - val_accuracy: 0.4873\n",
      "Epoch 8/20\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 1.2862 - accuracy: 0.5561 - val_loss: 1.4491 - val_accuracy: 0.4870\n",
      "Epoch 9/20\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 1.2834 - accuracy: 0.5577 - val_loss: 1.4485 - val_accuracy: 0.4905\n",
      "313/313 [==============================] - 1s 3ms/step\n",
      "Classification Report for lecun_normal:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      1017\n",
      "           1       0.10      0.30      0.15       990\n",
      "           2       0.00      0.00      0.00      1061\n",
      "           3       0.00      0.00      0.00       994\n",
      "           4       0.08      0.62      0.15       936\n",
      "           5       0.00      0.00      0.00      1009\n",
      "           6       0.00      0.00      0.00      1034\n",
      "           7       0.00      0.00      0.00      1005\n",
      "           8       0.00      0.00      0.00       956\n",
      "           9       0.00      0.00      0.00       998\n",
      "\n",
      "    accuracy                           0.09     10000\n",
      "   macro avg       0.02      0.09      0.03     10000\n",
      "weighted avg       0.02      0.09      0.03     10000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/deep_learning/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/envs/deep_learning/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/envs/deep_learning/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Define a list of different weight initializations to test\n",
    "initializations = ['glorot_uniform', 'he_normal', 'lecun_normal']\n",
    "\n",
    "# Loop through different weight initializations and train models\n",
    "for initialization in initializations:\n",
    "    # Create a feedforward neural net\n",
    "    newModel = Sequential()\n",
    "\n",
    "    # Create the layers with the chosen weight initialization\n",
    "    newModel.add(Flatten(input_shape=(32, 32, 3)))\n",
    "    newModel.add(Dense(256, activation='relu', kernel_regularizer = l1_l2(l1 = 0.0, l2 = 0.0001), kernel_initializer=initialization))\n",
    "    newModel.add(Dropout(0.5))\n",
    "    newModel.add(Dense(128, activation='relu', kernel_regularizer = l1_l2(l1 = 0.0, l2 = 0.0001), kernel_initializer=initialization))\n",
    "    newModel.add(Dropout(0.3))\n",
    "    newModel.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    # Compile the model\n",
    "    newModel.compile(loss='categorical_crossentropy', optimizer=SGD(learning_rate=learning_rate), metrics=['accuracy'])\n",
    "\n",
    "    # Train the model on the training data\n",
    "    model.fit(trainX, trainY, epochs=epochs, batch_size=batch_size, validation_data=(validX, validY), callbacks=[early_stopping])\n",
    "\n",
    "    # Use the trained model to make predictions on the valid set\n",
    "    valid_predictions = newModel.predict(validX)\n",
    "\n",
    "    # Convert predictions to class labels\n",
    "    valid_predictions_labels = np.argmax(valid_predictions, axis=1)\n",
    "    valid_true_labels = np.argmax(validY, axis=1)\n",
    "\n",
    "    # Generate a classification report for each initialization\n",
    "    classification_rep = classification_report(valid_true_labels, valid_predictions_labels)\n",
    "\n",
    "    # Print the classification report for each initialization\n",
    "    print(f\"Classification Report for {initialization}:\")\n",
    "    print(classification_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f433cbdb",
   "metadata": {},
   "source": [
    "Well that was unfortunate. All these initializers seemeed really promising, showing us upwards of 50% accuracy during the epochs. However, they all ended at awful 10% accuraccies. We are also seeing some warnings showing up in the terminal indicating a division by zero is occuring. This probably has to do with numbers being too small within logarithms, and honestly I don't know how to fix it, so we're just gonna accept this loss and move on. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b54d31",
   "metadata": {},
   "source": [
    "##### Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b55ed86",
   "metadata": {},
   "source": [
    "There are many ways we could modify the architecture, from adding convolutional layers to using a pre-trained model. However, a lot of these are beyond the scope of this project, so instead we'll simply add an additional hidden layer to see if that has an impact. This should increase the model's capacity to learn, however, it is important to keep in mind that our parameters might not be tuned for this additional layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "e8d3e16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a feedforward neural net with additional layer\n",
    "increasedModel = Sequential()\n",
    "\n",
    "# Create the layers\n",
    "increasedModel.add(Flatten(input_shape=(32,32,3)))\n",
    "increasedModel.add(Dense(256, activation = 'relu', kernel_regularizer = l1_l2(l1 = 0.0, l2 = 0.0001)))\n",
    "increasedModel.add(Dropout(0.5))\n",
    "increasedModel.add(Dense(128, activation = 'relu', kernel_regularizer = l1_l2(l1 = 0.0, l2 = 0.0001)))\n",
    "increasedModel.add(Dropout(0.3))\n",
    "increasedModel.add(Dense(64, activation = 'relu', kernel_regularizer = l1_l2(l1 = 0.0, l2 = 0.0001)))\n",
    "increasedModel.add(Dropout(0.1))\n",
    "increasedModel.add(Dense(10, activation = 'softmax', kernel_regularizer = l1_l2(l1 = 0.0, l2 = 0.0001)))\n",
    "\n",
    "# Compile the model\n",
    "increasedModel.compile(loss='categorical_crossentropy', optimizer=SGD(learning_rate=learning_rate), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "6fc97899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "469/469 [==============================] - 7s 12ms/step - loss: 2.4338 - accuracy: 0.1043 - val_loss: 2.3575 - val_accuracy: 0.1545\n",
      "Epoch 2/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 2.3687 - accuracy: 0.1234 - val_loss: 2.3317 - val_accuracy: 0.1817\n",
      "Epoch 3/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 2.3404 - accuracy: 0.1443 - val_loss: 2.3033 - val_accuracy: 0.1899\n",
      "Epoch 4/20\n",
      "469/469 [==============================] - 6s 14ms/step - loss: 2.3225 - accuracy: 0.1584 - val_loss: 2.2743 - val_accuracy: 0.2012\n",
      "Epoch 5/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 2.2957 - accuracy: 0.1722 - val_loss: 2.2411 - val_accuracy: 0.2066\n",
      "Epoch 6/20\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 2.2719 - accuracy: 0.1814 - val_loss: 2.2117 - val_accuracy: 0.2156\n",
      "Epoch 7/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 2.2497 - accuracy: 0.1855 - val_loss: 2.1863 - val_accuracy: 0.2360\n",
      "Epoch 8/20\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 2.2278 - accuracy: 0.1961 - val_loss: 2.1620 - val_accuracy: 0.2416\n",
      "Epoch 9/20\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 2.2106 - accuracy: 0.2023 - val_loss: 2.1397 - val_accuracy: 0.2659\n",
      "Epoch 10/20\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 2.1907 - accuracy: 0.2147 - val_loss: 2.1187 - val_accuracy: 0.2730\n",
      "Epoch 11/20\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 2.1730 - accuracy: 0.2192 - val_loss: 2.0987 - val_accuracy: 0.2774\n",
      "Epoch 12/20\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 2.1595 - accuracy: 0.2237 - val_loss: 2.0821 - val_accuracy: 0.2817\n",
      "Epoch 13/20\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 2.1422 - accuracy: 0.2317 - val_loss: 2.0673 - val_accuracy: 0.2948\n",
      "Epoch 14/20\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 2.1311 - accuracy: 0.2381 - val_loss: 2.0557 - val_accuracy: 0.3023\n",
      "Epoch 15/20\n",
      "469/469 [==============================] - 7s 15ms/step - loss: 2.1182 - accuracy: 0.2398 - val_loss: 2.0403 - val_accuracy: 0.3075\n",
      "Epoch 16/20\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 2.1089 - accuracy: 0.2493 - val_loss: 2.0295 - val_accuracy: 0.3154\n",
      "Epoch 17/20\n",
      "469/469 [==============================] - 9s 19ms/step - loss: 2.0970 - accuracy: 0.2513 - val_loss: 2.0146 - val_accuracy: 0.3147\n",
      "Epoch 18/20\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 2.0903 - accuracy: 0.2548 - val_loss: 2.0109 - val_accuracy: 0.3190\n",
      "Epoch 19/20\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 2.0829 - accuracy: 0.2585 - val_loss: 1.9987 - val_accuracy: 0.3239\n",
      "Epoch 20/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 2.0716 - accuracy: 0.2627 - val_loss: 1.9892 - val_accuracy: 0.3311\n",
      "313/313 [==============================] - 1s 3ms/step\n",
      "Classification Report for Default Model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.47      0.41      1017\n",
      "           1       0.47      0.16      0.24       990\n",
      "           2       0.25      0.23      0.24      1061\n",
      "           3       0.26      0.12      0.17       994\n",
      "           4       0.26      0.36      0.30       936\n",
      "           5       0.34      0.36      0.35      1009\n",
      "           6       0.31      0.33      0.32      1034\n",
      "           7       0.33      0.23      0.27      1005\n",
      "           8       0.41      0.47      0.44       956\n",
      "           9       0.36      0.58      0.44       998\n",
      "\n",
      "    accuracy                           0.33     10000\n",
      "   macro avg       0.34      0.33      0.32     10000\n",
      "weighted avg       0.33      0.33      0.32     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model on the training data\n",
    "increasedModel.fit(trainX, trainY, epochs=epochs, batch_size=batch_size, validation_data=(validX, validY), callbacks=[early_stopping])\n",
    "\n",
    "# Use the trained model to make predictions on the valid set\n",
    "valid_predictions = increasedModel.predict(validX)\n",
    "\n",
    "# Convert predictions to class labels\n",
    "valid_predictions_labels = np.argmax(valid_predictions, axis=1)\n",
    "valid_true_labels = np.argmax(validY, axis=1)\n",
    "\n",
    "# Generate a classification report\n",
    "classification_rep = classification_report(valid_true_labels, valid_predictions_labels)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification Report for Default Model:\")\n",
    "print(classification_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b1482d",
   "metadata": {},
   "source": [
    "Surprisingly enough, we see the model preforms worse here with an additional layer. We can suspect that this is due to the rest of the parameters not being a good fit, and in the interest of time we'll just stick to our previous model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe102617",
   "metadata": {},
   "source": [
    "##### Activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13ea9bd",
   "metadata": {},
   "source": [
    "Our current ReLU activation seems to be doing decently well, so maybe we can try a variation of it to seek improvements. For that we'll be using a Leaky ReLU. This fixes one of the main problems that a normal ReLU function faces, which is the \"dying ReLU\" problem, where neurons can get stuck during training and never activate again. This is done by allowing a small gradient for negative outputs, determined by the $\\alpha$ parameter. For the purposes of our experiment, we'll leave this $\\alpha$ as a very small number, `0.1`, for slight improvement. Note that the Leaky ReLU will only be applied to the itermediate layers, while the output remains as `softmax`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "d25e2770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a feedforward neural net with regularization\n",
    "model = Sequential()\n",
    "\n",
    "# Create the layers\n",
    "model.add(Flatten(input_shape=(32,32,3)))\n",
    "model.add(Dense(256, activation=LeakyReLU(alpha=0.1), kernel_regularizer = l1_l2(l1 = 0.0, l2 = 0.0001)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation=LeakyReLU(alpha=0.1), kernel_regularizer = l1_l2(l1 = 0.0, l2 = 0.0001)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(10, activation = 'softmax', kernel_regularizer = l1_l2(l1 = 0.0, l2 = 0.0001)))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=SGD(learning_rate=learning_rate), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "8520378f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "469/469 [==============================] - 8s 16ms/step - loss: 2.3758 - accuracy: 0.1418 - val_loss: 2.2457 - val_accuracy: 0.2396\n",
      "Epoch 2/20\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 2.2733 - accuracy: 0.1824 - val_loss: 2.1887 - val_accuracy: 0.2603\n",
      "Epoch 3/20\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 2.2259 - accuracy: 0.2047 - val_loss: 2.1434 - val_accuracy: 0.2775\n",
      "Epoch 4/20\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 2.1933 - accuracy: 0.2210 - val_loss: 2.1099 - val_accuracy: 0.2839\n",
      "Epoch 5/20\n",
      "469/469 [==============================] - 9s 19ms/step - loss: 2.1644 - accuracy: 0.2304 - val_loss: 2.0814 - val_accuracy: 0.2918\n",
      "Epoch 6/20\n",
      "469/469 [==============================] - 8s 17ms/step - loss: 2.1437 - accuracy: 0.2405 - val_loss: 2.0589 - val_accuracy: 0.3031\n",
      "Epoch 7/20\n",
      "469/469 [==============================] - 6s 14ms/step - loss: 2.1224 - accuracy: 0.2467 - val_loss: 2.0427 - val_accuracy: 0.3117\n",
      "Epoch 8/20\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 2.1058 - accuracy: 0.2535 - val_loss: 2.0239 - val_accuracy: 0.3213\n",
      "Epoch 9/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 2.0935 - accuracy: 0.2625 - val_loss: 2.0066 - val_accuracy: 0.3262\n",
      "Epoch 10/20\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 2.0740 - accuracy: 0.2694 - val_loss: 1.9927 - val_accuracy: 0.3313\n",
      "Epoch 11/20\n",
      "469/469 [==============================] - 7s 15ms/step - loss: 2.0636 - accuracy: 0.2726 - val_loss: 1.9809 - val_accuracy: 0.3343\n",
      "Epoch 12/20\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 2.0530 - accuracy: 0.2805 - val_loss: 1.9684 - val_accuracy: 0.3463\n",
      "Epoch 13/20\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 2.0403 - accuracy: 0.2850 - val_loss: 1.9600 - val_accuracy: 0.3462\n",
      "Epoch 14/20\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 2.0344 - accuracy: 0.2864 - val_loss: 1.9485 - val_accuracy: 0.3458\n",
      "Epoch 15/20\n",
      "469/469 [==============================] - 6s 14ms/step - loss: 2.0216 - accuracy: 0.2911 - val_loss: 1.9418 - val_accuracy: 0.3495\n",
      "Epoch 16/20\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 2.0153 - accuracy: 0.2954 - val_loss: 1.9314 - val_accuracy: 0.3557\n",
      "Epoch 17/20\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 2.0046 - accuracy: 0.2989 - val_loss: 1.9221 - val_accuracy: 0.3575\n",
      "Epoch 18/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 1.9987 - accuracy: 0.2999 - val_loss: 1.9154 - val_accuracy: 0.3633\n",
      "Epoch 19/20\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 1.9914 - accuracy: 0.3050 - val_loss: 1.9112 - val_accuracy: 0.3582\n",
      "Epoch 20/20\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 1.9865 - accuracy: 0.3114 - val_loss: 1.9062 - val_accuracy: 0.3624\n",
      "313/313 [==============================] - 1s 3ms/step\n",
      "Classification Report for New Activation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.38      0.41      1017\n",
      "           1       0.38      0.36      0.37       990\n",
      "           2       0.29      0.22      0.25      1061\n",
      "           3       0.30      0.15      0.20       994\n",
      "           4       0.29      0.35      0.32       936\n",
      "           5       0.34      0.39      0.36      1009\n",
      "           6       0.34      0.45      0.39      1034\n",
      "           7       0.44      0.27      0.33      1005\n",
      "           8       0.36      0.68      0.47       956\n",
      "           9       0.43      0.40      0.42       998\n",
      "\n",
      "    accuracy                           0.36     10000\n",
      "   macro avg       0.37      0.36      0.35     10000\n",
      "weighted avg       0.37      0.36      0.35     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model on the training data\n",
    "model.fit(trainX, trainY, epochs=epochs, batch_size=batch_size, validation_data=(validX, validY), callbacks=[early_stopping])\n",
    "\n",
    "# Use the trained model to make predictions on the valid set\n",
    "valid_predictions = model.predict(validX)\n",
    "\n",
    "# Convert predictions to class labels\n",
    "valid_predictions_labels = np.argmax(valid_predictions, axis=1)\n",
    "valid_true_labels = np.argmax(validY, axis=1)\n",
    "\n",
    "# Generate a classification report\n",
    "classification_rep = classification_report(valid_true_labels, valid_predictions_labels)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification Report for New Activation:\")\n",
    "print(classification_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cd3f43",
   "metadata": {},
   "source": [
    "Not much improvement to be seen here. We've achieved similar accuracy as before without overfitting so that is a plus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41efc30",
   "metadata": {},
   "source": [
    "Let's try PReLU, which is similar to LeakyReLU, with the key difference being the slope of the negative part of the function is learned during training, rather than being a fixed hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "5162da17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a feedforward neural net with regularization\n",
    "model = Sequential()\n",
    "\n",
    "# Create the layers\n",
    "model.add(Flatten(input_shape=(32,32,3)))\n",
    "model.add(Dense(256, activation=PReLU(), kernel_regularizer = l1_l2(l1 = 0.0, l2 = 0.0001)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation=PReLU(), kernel_regularizer = l1_l2(l1 = 0.0, l2 = 0.0001)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(10, activation = 'softmax', kernel_regularizer = l1_l2(l1 = 0.0, l2 = 0.0001)))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=SGD(learning_rate=learning_rate), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "4d706096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "469/469 [==============================] - 9s 16ms/step - loss: 2.3938 - accuracy: 0.1320 - val_loss: 2.2578 - val_accuracy: 0.2227\n",
      "Epoch 2/20\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 2.2766 - accuracy: 0.1785 - val_loss: 2.1977 - val_accuracy: 0.2549\n",
      "Epoch 3/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 2.2319 - accuracy: 0.1978 - val_loss: 2.1545 - val_accuracy: 0.2659\n",
      "Epoch 4/20\n",
      "469/469 [==============================] - 5s 12ms/step - loss: 2.2032 - accuracy: 0.2135 - val_loss: 2.1220 - val_accuracy: 0.2819\n",
      "Epoch 5/20\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 2.1706 - accuracy: 0.2249 - val_loss: 2.0939 - val_accuracy: 0.2828\n",
      "Epoch 6/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 2.1528 - accuracy: 0.2357 - val_loss: 2.0709 - val_accuracy: 0.2964\n",
      "Epoch 7/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 2.1338 - accuracy: 0.2426 - val_loss: 2.0508 - val_accuracy: 0.3095\n",
      "Epoch 8/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 2.1130 - accuracy: 0.2476 - val_loss: 2.0362 - val_accuracy: 0.3089\n",
      "Epoch 9/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 2.0987 - accuracy: 0.2561 - val_loss: 2.0160 - val_accuracy: 0.3173\n",
      "Epoch 10/20\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 2.0884 - accuracy: 0.2609 - val_loss: 2.0023 - val_accuracy: 0.3226\n",
      "Epoch 11/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 2.0738 - accuracy: 0.2673 - val_loss: 1.9899 - val_accuracy: 0.3310\n",
      "Epoch 12/20\n",
      "469/469 [==============================] - 5s 12ms/step - loss: 2.0590 - accuracy: 0.2766 - val_loss: 1.9719 - val_accuracy: 0.3339\n",
      "Epoch 13/20\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 2.0495 - accuracy: 0.2775 - val_loss: 1.9640 - val_accuracy: 0.3389\n",
      "Epoch 14/20\n",
      "469/469 [==============================] - 5s 12ms/step - loss: 2.0429 - accuracy: 0.2786 - val_loss: 1.9527 - val_accuracy: 0.3452\n",
      "Epoch 15/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 2.0304 - accuracy: 0.2869 - val_loss: 1.9431 - val_accuracy: 0.3479\n",
      "Epoch 16/20\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 2.0225 - accuracy: 0.2900 - val_loss: 1.9327 - val_accuracy: 0.3487\n",
      "Epoch 17/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 2.0190 - accuracy: 0.2940 - val_loss: 1.9279 - val_accuracy: 0.3522\n",
      "Epoch 18/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 2.0060 - accuracy: 0.2922 - val_loss: 1.9181 - val_accuracy: 0.3555\n",
      "Epoch 19/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 1.9986 - accuracy: 0.3048 - val_loss: 1.9103 - val_accuracy: 0.3579\n",
      "Epoch 20/20\n",
      "469/469 [==============================] - 6s 14ms/step - loss: 1.9910 - accuracy: 0.3035 - val_loss: 1.9004 - val_accuracy: 0.3605\n",
      "313/313 [==============================] - 1s 3ms/step\n",
      "Classification Report for New Activation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.41      0.40      1017\n",
      "           1       0.40      0.39      0.39       990\n",
      "           2       0.28      0.21      0.24      1061\n",
      "           3       0.27      0.17      0.21       994\n",
      "           4       0.29      0.30      0.30       936\n",
      "           5       0.33      0.38      0.35      1009\n",
      "           6       0.34      0.44      0.38      1034\n",
      "           7       0.44      0.26      0.33      1005\n",
      "           8       0.40      0.64      0.49       956\n",
      "           9       0.42      0.41      0.42       998\n",
      "\n",
      "    accuracy                           0.36     10000\n",
      "   macro avg       0.36      0.36      0.35     10000\n",
      "weighted avg       0.36      0.36      0.35     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model on the training data\n",
    "model.fit(trainX, trainY, epochs=epochs, batch_size=batch_size, validation_data=(validX, validY), callbacks=[early_stopping])\n",
    "\n",
    "# Use the trained model to make predictions on the valid set\n",
    "valid_predictions = model.predict(validX)\n",
    "\n",
    "# Convert predictions to class labels\n",
    "valid_predictions_labels = np.argmax(valid_predictions, axis=1)\n",
    "valid_true_labels = np.argmax(validY, axis=1)\n",
    "\n",
    "# Generate a classification report\n",
    "classification_rep = classification_report(valid_true_labels, valid_predictions_labels)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification Report for New Activation:\")\n",
    "print(classification_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5df2a1",
   "metadata": {},
   "source": [
    "Not much improvement here, but it was worth a try either way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51b2a6d",
   "metadata": {},
   "source": [
    "##### Loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4cf30e",
   "metadata": {},
   "source": [
    "Previously, we were using the `categorical_crossentropy` loss function, which is known to work well with the dataset we are using. However, we can also try modifying the loss function to see if there is any effect on the accuracy. For our experiment, we will be using `MeanSquaredError` loss function. This loss function is generally used for regression, but can work with classification as well so it may suit our needs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "0794fd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a feedforward neural net with regularization\n",
    "model = Sequential()\n",
    "\n",
    "# Create the layers\n",
    "model.add(Flatten(input_shape=(32,32,3)))\n",
    "model.add(Dense(256, activation = 'relu', kernel_regularizer = l1_l2(l1 = 0.0, l2 = 0.0001)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation = 'relu', kernel_regularizer = l1_l2(l1 = 0.0, l2 = 0.0001)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(10, activation = 'softmax', kernel_regularizer = l1_l2(l1 = 0.0, l2 = 0.0001)))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=SGD(learning_rate=learning_rate), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "6f0610e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "469/469 [==============================] - 6s 11ms/step - loss: 0.1636 - accuracy: 0.0973 - val_loss: 0.1574 - val_accuracy: 0.1077\n",
      "Epoch 2/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.1621 - accuracy: 0.1006 - val_loss: 0.1569 - val_accuracy: 0.1129\n",
      "Epoch 3/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.1610 - accuracy: 0.1048 - val_loss: 0.1565 - val_accuracy: 0.1142\n",
      "Epoch 4/20\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.1602 - accuracy: 0.1017 - val_loss: 0.1562 - val_accuracy: 0.1192\n",
      "Epoch 5/20\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.1594 - accuracy: 0.1039 - val_loss: 0.1559 - val_accuracy: 0.1243\n",
      "Epoch 6/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.1587 - accuracy: 0.1087 - val_loss: 0.1557 - val_accuracy: 0.1336\n",
      "Epoch 7/20\n",
      "469/469 [==============================] - 5s 12ms/step - loss: 0.1583 - accuracy: 0.1070 - val_loss: 0.1556 - val_accuracy: 0.1421\n",
      "Epoch 8/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.1581 - accuracy: 0.1078 - val_loss: 0.1555 - val_accuracy: 0.1455\n",
      "Epoch 9/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.1576 - accuracy: 0.1125 - val_loss: 0.1554 - val_accuracy: 0.1494\n",
      "Epoch 10/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.1573 - accuracy: 0.1178 - val_loss: 0.1553 - val_accuracy: 0.1528\n",
      "Epoch 11/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.1571 - accuracy: 0.1164 - val_loss: 0.1552 - val_accuracy: 0.1569\n",
      "Epoch 12/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.1570 - accuracy: 0.1148 - val_loss: 0.1551 - val_accuracy: 0.1616\n",
      "Epoch 13/20\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.1567 - accuracy: 0.1185 - val_loss: 0.1550 - val_accuracy: 0.1647\n",
      "Epoch 14/20\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.1567 - accuracy: 0.1182 - val_loss: 0.1550 - val_accuracy: 0.1689\n",
      "Epoch 15/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.1565 - accuracy: 0.1224 - val_loss: 0.1549 - val_accuracy: 0.1705\n",
      "Epoch 16/20\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.1562 - accuracy: 0.1283 - val_loss: 0.1548 - val_accuracy: 0.1755\n",
      "Epoch 17/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.1562 - accuracy: 0.1266 - val_loss: 0.1548 - val_accuracy: 0.1785\n",
      "Epoch 18/20\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.1561 - accuracy: 0.1258 - val_loss: 0.1547 - val_accuracy: 0.1815\n",
      "Epoch 19/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.1559 - accuracy: 0.1302 - val_loss: 0.1546 - val_accuracy: 0.1844\n",
      "Epoch 20/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.1559 - accuracy: 0.1289 - val_loss: 0.1546 - val_accuracy: 0.1867\n",
      "313/313 [==============================] - 1s 3ms/step\n",
      "Classification Report with Mean Squared Error Loss:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.16      0.34      0.22      1017\n",
      "           1       0.12      0.06      0.08       990\n",
      "           2       0.14      0.10      0.11      1061\n",
      "           3       0.16      0.07      0.10       994\n",
      "           4       0.09      0.03      0.04       936\n",
      "           5       0.21      0.32      0.25      1009\n",
      "           6       0.16      0.07      0.10      1034\n",
      "           7       0.24      0.19      0.21      1005\n",
      "           8       0.21      0.60      0.31       956\n",
      "           9       0.25      0.12      0.16       998\n",
      "\n",
      "    accuracy                           0.19     10000\n",
      "   macro avg       0.17      0.19      0.16     10000\n",
      "weighted avg       0.17      0.19      0.16     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compile the model with MeanSquaredError as the loss function\n",
    "model.compile(loss=MeanSquaredError(), optimizer=SGD(learning_rate=learning_rate), metrics=['accuracy'])\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(trainX, trainY, epochs=epochs, batch_size=batch_size, validation_data=(validX, validY), callbacks=[early_stopping])\n",
    "\n",
    "# Use the trained model to make predictions on the valid set\n",
    "valid_predictions = model.predict(validX)\n",
    "\n",
    "# Convert predictions to class labels\n",
    "valid_predictions_labels = np.argmax(valid_predictions, axis=1)\n",
    "valid_true_labels = np.argmax(validY, axis=1)\n",
    "\n",
    "# Generate a classification report\n",
    "classification_rep = classification_report(valid_true_labels, valid_predictions_labels)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification Report with Mean Squared Error Loss:\")\n",
    "print(classification_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d37690",
   "metadata": {},
   "source": [
    "A significant accuracy decrease here, which is to be expected as categorical crossentropy is known to be the better loss function in this usage case. Still very happy to see up to this point that our regularization is keeping the overfitting in check throughout the process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56a2626",
   "metadata": {},
   "source": [
    "##### Final Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a07b50",
   "metadata": {},
   "source": [
    "After all our trials, we found the following to work best:\n",
    "* SGD Learning Rate: 0.001\n",
    "* Batch Size: 64\n",
    "* Normalization\n",
    "* Regularization: $L^2$ = 0.0001 with 0.5 and 0.3 dropout\n",
    "* Activation: `ReLU`\n",
    "* Loss Function: `categorical_crossentropy`\n",
    "\n",
    "All of this achieved a best case accuracy of 37%. Overall not an accuracy to write home about, but keeping the model in check and preventing overfitting throughout is definitely something to be content with. Given more time, I would have experimented more with various parameters such as architecture and initialization which I believe have potential to bring the accuracy all the way up to at least a 50%. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
