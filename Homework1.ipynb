{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4744962-cf2d-414d-acf8-38e56a8c5bea",
   "metadata": {
    "tags": []
   },
   "source": [
    "# MTH 4320 / 5320 - Homework 1\n",
    "\n",
    "## Gradient Descent, Linear Models, and Logistic Classification\n",
    "\n",
    "**Deadline**: Sept 17\n",
    "\n",
    "**Points**: 100\n",
    "\n",
    "### Instructions\n",
    "\n",
    "Submit **one** Python notebook file for grading. Your file must include **text explanations** of your work, **well-commented code**, and the **outputs** from your code.\n",
    "\n",
    "### Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d689bc13",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26e5056",
   "metadata": {},
   "source": [
    "#### Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "11ca3799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6572fe64",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2042a8f",
   "metadata": {},
   "source": [
    "1. [10 points] Write a version of gradient descent with an option to use the explicit gradient formula of your loss function OR the `estimate_gradient` approximator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03004da9",
   "metadata": {},
   "source": [
    "We start by defining the `estimate_gradient` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "c022fd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate the gradient\n",
    "def estimate_Gradient(f, x, h):\n",
    "    n = len(x)\n",
    "    gradient = np.zeros(n)\n",
    "    \n",
    "    for counter in range(n):\n",
    "        xUp = x.copy()\n",
    "        xUp[counter] += h\n",
    "        gradient[counter] = (f(xUp) - f(x))/h\n",
    "            \n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8cfe2b",
   "metadata": {},
   "source": [
    "We then need to define a seperate function for the explicit gradient formula can mathematically be expressed as $\\nabla L (\\theta) = X^T X \\theta - X^T y$. Note that in our code, $\\theta$ will be represented by $w$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "5101e878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explicitly calculate the gradient\n",
    "def explicit_Gradient(x, w, y):\n",
    "    # convert to np array\n",
    "    X = np.array(x)\n",
    "    W = np.array(w)\n",
    "    Y = np.array(y)\n",
    "    \n",
    "    # check for 1d arrays, which can't handle transpose\n",
    "    if X.ndim == 1:\n",
    "        gradient = (X * X * W) - (X * Y)\n",
    "    else:\n",
    "        gradient = (X.T @ X @ W) - (X.T @ Y)\n",
    "\n",
    "    return np.array(gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5138e8c0",
   "metadata": {},
   "source": [
    "Next, we can implement the original `gradientDescent` function, but include an additional boolean paramter `explicit` which will determine whether or not we want to use the explicit gradient formula, or default to the gradient estimate. We also need to be sure to pass the $y$ and $\\theta$ values through the function so that `explicit_Gradient` can access them.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "98a3b9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run gradient descent and output the coordinates of the estimated critical point, checking for explicit gradient\n",
    "def gradientDescent(f, x0, alpha, h, tolerance, maxIterations, w, y, explicit=False):\n",
    "    # set x equal to the initial guess\n",
    "    x = x0 \n",
    "\n",
    "    # take up to maxIterations number of steps\n",
    "    for counter in range(maxIterations):\n",
    "        # update the gradient, checking the explicit parameter to determine the gradient method\n",
    "        if explicit:\n",
    "            gradient = explicit_Gradient(x, w, y)\n",
    "        else:\n",
    "            gradient = estimate_Gradient(f, x, h)\n",
    "        \n",
    "        # stop if the norm of the gradient is near 0 (success)\n",
    "        if np.linalg.norm(gradient) < tolerance:\n",
    "            print('Gradient descent took', counter, 'iterations to converge')\n",
    "            print('The norm of the gradient is', np.linalg.norm(gradient))\n",
    "            \n",
    "            # return the approximate critical point x\n",
    "            return x\n",
    "        \n",
    "        # print a message if we do not converge (failure)\n",
    "        elif counter == maxIterations-1:\n",
    "            print(\"Gradient descent failed\")\n",
    "            print('The gradient is', gradient)\n",
    "            \n",
    "            # return x, sometimes it is still pretty good\n",
    "            return x\n",
    "        \n",
    "        # take a step in the opposite direction as the gradient\n",
    "        x -= alpha*gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b70d337",
   "metadata": {},
   "source": [
    "Let's try this out using $x^2$ as an example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "4abd61e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test x^2\n",
    "f = lambda x : x[0]**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "94e604d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient descent took 10 iterations to converge\n",
      "The norm of the gradient is 4.5055999998641627e-07\n",
      "[-0.19999977] 0.03999990988805076\n"
     ]
    }
   ],
   "source": [
    "# using gradient estimate\n",
    "x = gradientDescent(f,[2],0.4,0.4,0.000001,10000, [0.1, 0.2], [2])\n",
    "print(x, f(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "43ed66f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient descent took 16 iterations to converge\n",
      "The norm of the gradient is 3.0908304814264935e-07\n",
      "[19.99999985  9.99999999] 399.9999938305237\n"
     ]
    }
   ],
   "source": [
    "# using explicit gradient\n",
    "x = gradientDescent(f,[2],0.4,0.4,0.000001,10000, [0.1, 0.2], [2], explicit=True)\n",
    "print(x, f(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b59470",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda647a1",
   "metadata": {},
   "source": [
    "2. [10 points] Write a version of gradient descent that chooses $n$ random starting points and outputs the parameters resulting in minimum training loss across all the runs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25694d5a",
   "metadata": {},
   "source": [
    "Let's add an input parameter to the`gradientDescent` function so that it selects $n$ starting points instead of just one x0 value. Then, inside the function, let's use that value $n$ to iterate through $n$ different randomly selected starting points. Finally, as we iterate through these points, lets save the best loss functions. Note: we can also remove all the previous return functions, since we do not want to output each individual run point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "4c1e5735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run gradient descent for n random starting points, and output the parameters resulting in min training loss\n",
    "def randomGradientDescent(f, x0, alpha, h, tolerance, maxIterations, w, y, n, explicit=False):\n",
    "    # declare best parameters\n",
    "    best_loss = float('inf')\n",
    "    best_x = 0\n",
    "    \n",
    "    # iterate through n points\n",
    "    for i in range(n):\n",
    "        # randomly selecting starting value \n",
    "        x = np.random.rand(len(x0))\n",
    "\n",
    "        # take up to maxIterations number of steps\n",
    "        for counter in range(maxIterations):\n",
    "\n",
    "            # update the gradient, checking the explicit parameter to determine the gradient method\n",
    "            if explicit:\n",
    "                gradient = explicit_Gradient(f, x, w, y)\n",
    "            else:\n",
    "                gradient = estimate_Gradient(f, x, h)\n",
    "\n",
    "            # stop if the norm of the gradient is near 0 (success)\n",
    "            if np.linalg.norm(gradient) < tolerance:\n",
    "                break\n",
    "\n",
    "            # take a step in the opposite direction as the gradient\n",
    "            x -= alpha*gradient\n",
    "        \n",
    "        # store the current loss\n",
    "        loss = f(x)\n",
    "        \n",
    "        # update the best parameters\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            best_x = x\n",
    "    \n",
    "    # return the overall best values\n",
    "    return best_loss, best_x   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6497e752",
   "metadata": {},
   "source": [
    "Let's try this out using $x^2$ as an example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "be3ad7f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best loss was 0.03999980313185498, achieved with an x value of [-0.19999951].\n"
     ]
    }
   ],
   "source": [
    "# using gradient estimate\n",
    "loss, x = randomGradientDescent(f,[2],0.4,0.4,0.000001,10000, [0.1, 0.2], [2], 10)\n",
    "print(f\"The best loss was {loss}, achieved with an x value of {x}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514d62c2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90a0967",
   "metadata": {},
   "source": [
    "3. [10 points] Write a version with a **cyclic** learning rate that changes in each training epoch and saves the model parameters every time the learning rate vanishes. Then, select the best parameters observed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "da6877a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run gradient descent for n random starting points with cyclic learning, and output the best parameters\n",
    "def cyclicGradientDescent(f, x0, alpha, h, tolerance, maxIterations, w, y, n, explicit=False):\n",
    "    # declare best parameters\n",
    "    best_loss = float('inf')\n",
    "    best_x = 0\n",
    "    best_alpha = 0\n",
    "    \n",
    "    # iterate through n points\n",
    "    for i in range(n):\n",
    "        # randomly selecting starting value \n",
    "        x = np.random.rand(len(x0))\n",
    "        \n",
    "        # reset learning rate\n",
    "        learn_rate = alpha \n",
    "\n",
    "        # take up to maxIterations number of steps\n",
    "        for counter in range(maxIterations):\n",
    "\n",
    "            # update the gradient, checking the explicit parameter to determine the gradient method\n",
    "            if explicit:\n",
    "                gradient = explicit_Gradient(f, x, w, y)\n",
    "            else:\n",
    "                gradient = estimate_Gradient(f, x, h)\n",
    "\n",
    "            # stop if the norm of the gradient is near 0 (success)\n",
    "            if np.linalg.norm(gradient) < tolerance:\n",
    "                break\n",
    "\n",
    "            # take a step in the opposite direction as the gradient\n",
    "            x -= alpha*gradient\n",
    "        \n",
    "        # store the current loss\n",
    "        loss = f(x)\n",
    "        \n",
    "        # update the best parameters\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            best_x = x\n",
    "            best_alpha = learn_rate\n",
    "            \n",
    "        # linearly decrease the learning rate\n",
    "        learn_rate -= alpha / maxIterations\n",
    "        \n",
    "        # stop if the learning rate vanishes\n",
    "        if learn_rate < 0:\n",
    "            break\n",
    "    \n",
    "    # return the overall best values\n",
    "    return best_loss, best_x, best_alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1625883",
   "metadata": {},
   "source": [
    "Let's try this out using $x^2$ as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "0d116911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best loss was 0.03999981312859259, achieved with an x value of [-0.19999953] and a learning rate of 0.4.\n"
     ]
    }
   ],
   "source": [
    "# using gradient estimate\n",
    "loss, x, alpha = cyclicGradientDescent(f,[2],0.4,0.4,0.000001,10000, [0.1, 0.2], [2], 10)\n",
    "print(f\"The best loss was {loss}, achieved with an x value of {x} and a learning rate of {alpha}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db789b8f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c8d001",
   "metadata": {},
   "source": [
    "4. [10 points] For the gradient descent variants from problems 2-3, instead of saving only the best parameters, save all sets of parameters that are converged to, predict outputs using each set of parameters, and then have final output predictions equal to the mean of the model predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde33a41",
   "metadata": {},
   "source": [
    "For the gradient descent in problem 2: \n",
    "\\\n",
    "\\\n",
    "Instead of storing the values for losses, let's store the value from converges when the gradient is near 0. Then, to predict the output we simply take our saved values and pass them through our `f(x)` function, and finally we return the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "954b75d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run gradient descent for n random starting points, and output the parameters resulting in min training loss\n",
    "def randomMeanGradientDescent(f, x0, alpha, h, tolerance, maxIterations, w, y, n, explicit=False):\n",
    "    # declare converged parameters\n",
    "    converged = []\n",
    "    \n",
    "    # iterate through n points\n",
    "    for i in range(n):\n",
    "        # randomly selecting starting value \n",
    "        x = np.random.rand(len(x0))\n",
    "\n",
    "        # take up to maxIterations number of steps\n",
    "        for counter in range(maxIterations):\n",
    "\n",
    "            # update the gradient, checking the explicit parameter to determine the gradient method\n",
    "            if explicit:\n",
    "                gradient = explicit_Gradient(f, x, w, y)\n",
    "            else:\n",
    "                gradient = estimate_Gradient(f, x, h)\n",
    "\n",
    "            # stop if the norm of the gradient is near 0 (success)\n",
    "            if np.linalg.norm(gradient) < tolerance:\n",
    "                converged.append(x)\n",
    "                break\n",
    "\n",
    "            # take a step in the opposite direction as the gradient\n",
    "            x -= alpha*gradient\n",
    "    \n",
    "    # declare predicted outputs\n",
    "    predicted = [f(x)]\n",
    "       \n",
    "    # return the mean     \n",
    "    return np.mean(predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca54f02",
   "metadata": {},
   "source": [
    "For the gradient descent in problem 3:\n",
    "\\\n",
    "\\\n",
    "Let's follow the same approach as previously, where we store the converged values rather than the best loss values. Once again, finding the predicted values simply requires passing the `f(x)` function, and the mean is returned at the end. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "1a0bbe45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run gradient descent for n random starting points with cyclic learning, and output the best parameters\n",
    "def cyclicMeanGradientDescent(f, x0, alpha, h, tolerance, maxIterations, w, y, n, explicit=False):\n",
    "    # declare converged parameters\n",
    "    converged = []\n",
    "    \n",
    "    # iterate through n points\n",
    "    for i in range(n):\n",
    "        # randomly selecting starting value \n",
    "        x = np.random.rand(len(x0))\n",
    "        \n",
    "        # reset learning rate\n",
    "        learn_rate = alpha \n",
    "\n",
    "        # take up to maxIterations number of steps\n",
    "        for counter in range(maxIterations):\n",
    "\n",
    "            # update the gradient, checking the explicit parameter to determine the gradient method\n",
    "            if explicit:\n",
    "                gradient = explicit_Gradient(f, x, w, y)\n",
    "            else:\n",
    "                gradient = estimate_Gradient(f, x, h)\n",
    "\n",
    "            # stop if the norm of the gradient is near 0 (success)\n",
    "            if np.linalg.norm(gradient) < tolerance:\n",
    "                converged.append(x)\n",
    "                break\n",
    "\n",
    "            # take a step in the opposite direction as the gradient\n",
    "            x -= alpha*gradient\n",
    "            \n",
    "        # linearly decrease the learning rate\n",
    "        learn_rate -= alpha / maxIterations\n",
    "        \n",
    "        # stop if the learning rate vanishes\n",
    "        if learn_rate < 0:\n",
    "            break\n",
    "    \n",
    "    # declare predicted outputs\n",
    "    predicted = [f(x)]\n",
    "       \n",
    "    # return the mean     \n",
    "    return np.mean(predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18339a43",
   "metadata": {},
   "source": [
    "Let's try both of these out using $x^2$ as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "68fbfd8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean of the model predictions using randomized gradient descent is 0.039999878350889584.\n"
     ]
    }
   ],
   "source": [
    "# using gradient estimate\n",
    "mean = randomMeanGradientDescent(f,[2],0.4,0.4,0.000001,10000, [0.1, 0.2], [2], 10)\n",
    "print(f\"The mean of the model predictions using randomized gradient descent is {mean}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "d93b10e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean of the model predictions using a cyclic learning rate is 0.039999903404734714.\n"
     ]
    }
   ],
   "source": [
    "# using gradient estimate\n",
    "mean = cyclicMeanGradientDescent(f,[2],0.4,0.4,0.000001,10000, [0.1, 0.2], [2], 10)\n",
    "print(f\"The mean of the model predictions using a cyclic learning rate is {mean}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139ba401",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c522b5",
   "metadata": {},
   "source": [
    "5. [15 points] Apply your model with all six variants of gradient descent (3 variants with estimated gradient and 3 variants with exact gradient) to predict house prices in the [Mount Pleasant Real Estate Dataset](https://www.hawkeslearning.com/Statistics/dis/datasets.html) using columns C-O, Q-T, and V-W (note some columns will need to be converted to binary or one-hot representations). Compare the results in terms of test accuracy, `fit` runtime, and `predict` runtime. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde83b04",
   "metadata": {},
   "source": [
    "Let's begin by importing our data from a csv file. Credit goes to Gabby Pang for providing an outline of this code on Discord for other students to use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "d47448fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data as csv into a pandas file\n",
    "df = pd.read_csv('/Users/andreweden/Desktop/deep_learning/Mount_Pleasant_Real_Estate_Data.csv')\n",
    "\n",
    "# Remove NA rows\n",
    "df = df.dropna(axis='rows')\n",
    "\n",
    "# Remove unused collumns\n",
    "df = df.drop(['ID','Misc Exterior', 'Amenities', 'Number of Fireplaces'], axis=1)\n",
    "\n",
    "# Apply one-hot encoding\n",
    "df = pd.get_dummies(df, columns = ['Subdivision', 'House Style'])\n",
    "\n",
    "# Convert yes/no to binary\n",
    "yes_no = ['Duplex?','New Owned?', 'Has Pool?', 'Has Dock?', 'Fenced Yard', 'Screened Porch?', 'Golf Course?', 'Fireplace?']\n",
    "for i in yes_no:\n",
    "    df[i].replace({\"Yes\": 1, \"No\": 0}, inplace=True)\n",
    "    \n",
    "# Remove commas and dollar signs\n",
    "df[\"List Price\"] = df[\"List Price\"].str.replace(',', '').str.replace('$', '').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "c26b17d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>List Price</th>\n",
       "      <th>Duplex?</th>\n",
       "      <th>Bedrooms</th>\n",
       "      <th>Baths - Total</th>\n",
       "      <th>Baths - Full</th>\n",
       "      <th>Baths - Half</th>\n",
       "      <th>Stories</th>\n",
       "      <th>Square Footage</th>\n",
       "      <th>Year Built</th>\n",
       "      <th>Acreage</th>\n",
       "      <th>...</th>\n",
       "      <th>House Style_Colonial</th>\n",
       "      <th>House Style_Condo Regime</th>\n",
       "      <th>House Style_Condominium</th>\n",
       "      <th>House Style_Contemporary</th>\n",
       "      <th>House Style_Cottage</th>\n",
       "      <th>House Style_Craftsman</th>\n",
       "      <th>House Style_Patio</th>\n",
       "      <th>House Style_Ranch</th>\n",
       "      <th>House Style_Townhouse</th>\n",
       "      <th>House Style_Traditional</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>369900</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1797.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>0.06</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>375000</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1797.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>0.06</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>769900</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2767.0</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>0.35</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>699990</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3240.0</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>0.29</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>436625</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2072.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>0.19</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   List Price  Duplex?  Bedrooms  Baths - Total  Baths - Full  Baths - Half  \\\n",
       "0      369900        1       3.0            2.5           2.0           1.0   \n",
       "1      375000        1       3.0            2.5           2.0           1.0   \n",
       "2      769900        0       4.0            3.5           3.0           1.0   \n",
       "3      699990        0       4.0            3.5           3.0           1.0   \n",
       "4      436625        0       4.0            3.0           3.0           0.0   \n",
       "\n",
       "   Stories  Square Footage  Year Built  Acreage  ...  House Style_Colonial  \\\n",
       "0      2.0          1797.0      2017.0     0.06  ...                 False   \n",
       "1      2.0          1797.0      2017.0     0.06  ...                 False   \n",
       "2      2.0          2767.0      2014.0     0.35  ...                 False   \n",
       "3      2.0          3240.0      2014.0     0.29  ...                 False   \n",
       "4      2.0          2072.0      2017.0     0.19  ...                 False   \n",
       "\n",
       "   House Style_Condo Regime  House Style_Condominium  \\\n",
       "0                     False                    False   \n",
       "1                     False                    False   \n",
       "2                     False                    False   \n",
       "3                     False                    False   \n",
       "4                     False                    False   \n",
       "\n",
       "   House Style_Contemporary  House Style_Cottage  House Style_Craftsman  \\\n",
       "0                     False                False                  False   \n",
       "1                     False                False                  False   \n",
       "2                     False                False                  False   \n",
       "3                     False                False                  False   \n",
       "4                     False                False                  False   \n",
       "\n",
       "   House Style_Patio  House Style_Ranch  House Style_Townhouse  \\\n",
       "0              False              False                   True   \n",
       "1              False              False                   True   \n",
       "2              False              False                  False   \n",
       "3              False              False                  False   \n",
       "4              False              False                  False   \n",
       "\n",
       "   House Style_Traditional  \n",
       "0                    False  \n",
       "1                    False  \n",
       "2                     True  \n",
       "3                     True  \n",
       "4                     True  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the first rows for visual confirmation\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35ef923",
   "metadata": {},
   "source": [
    "Now we must define our 'fit' and 'predict' functions, which we can store in a logistic classifier class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "f937cffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticClassifierGradient:\n",
    "    \n",
    "    # initialize and select the gradient type\n",
    "    def __init__(self, gradientType=None, explicit=False):\n",
    "        self.gradientType = gradientType\n",
    "        self.explicit = explicit\n",
    "        \n",
    "    # fit the model to the data\n",
    "    def fit(self, X, y, w0, alpha, h, tolerance, max_iterations):\n",
    "\n",
    "        # save the training data\n",
    "        X = np.hstack((np.ones([X.shape[0], 1]), X))\n",
    "\n",
    "        # standardize the data\n",
    "        X = scale(X)\n",
    "\n",
    "        # find the w values that minimize the sum of squared errors via gradient descent\n",
    "        L = lambda w: (expit(X @ w).T - y.T) @ (expit(X @ w) - y)\n",
    "        \n",
    "        # find gradient based on gradient type\n",
    "        if self.gradientType == 'random':\n",
    "            randomGradientDescent(L, w0, alpha, h, tolerance, maxIterations, w0, y, n, explicit=self.explicit)\n",
    "        elif self.gradientType == 'cyclic':\n",
    "            cyclicGradientDescent(L, w0, alpha, h, tolerance, maxIterations, w0, y, n, explicit=self.explicit)\n",
    "        else:\n",
    "            gradientDescent(L, w0, alpha, h, tolerance, maxIterations, w0, y, explicit=self.explicit)\n",
    "            \n",
    "\n",
    "    # predict the output from testing data\n",
    "    def predict(self, X):        \n",
    "        # append a column of ones at the beginning of X\n",
    "        X = np.hstack((np.ones([X.shape[0],1]), X))\n",
    "\n",
    "        # standardize the data\n",
    "        X = scale(X)\n",
    "\n",
    "        # return the predicted y's\n",
    "        return expit(X @ self.w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4587bc",
   "metadata": {},
   "source": [
    "Finally, let's put this all together with our past gradient descent methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "41d24fbb",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[276], line 13\u001b[0m\n\u001b[1;32m      9\u001b[0m model \u001b[38;5;241m=\u001b[39m LogisticClassifierGradient()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# fit the Bayes classifier to the training data \u001b[39;00m\n\u001b[1;32m     12\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(trainX, trainY,\n\u001b[0;32m---> 13\u001b[0m           [\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m (X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m, h\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m,\n\u001b[1;32m     14\u001b[0m           tolerance\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m, max_iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# predict the Labels of the training set \u001b[39;00m\n\u001b[1;32m     17\u001b[0m predictedY \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrint(model\u001b[38;5;241m.\u001b[39mpredict(trainX))\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "# find the data and labels\n",
    "X = df['List Price']\n",
    "Y = df.drop('List Price', axis=1)\n",
    "\n",
    "# do a train/test split \n",
    "trainX, testX, trainY, testY = train_test_split(X, Y, test_size=0.25)\n",
    "\n",
    "# build the logistic classifier\n",
    "model = LogisticClassifierGradient()\n",
    "\n",
    "# fit the Bayes classifier to the training data \n",
    "model.fit(trainX, trainY,\n",
    "          [0] * (X.shape[1] + 1), alpha=0.001, h=0.001,\n",
    "          tolerance=0.001, max_iterations=10000)\n",
    "\n",
    "# predict the Labels of the training set \n",
    "predictedY = np.rint(model.predict(trainX))\n",
    "\n",
    "# print quality metrics \n",
    "print(f'\\nTrain Classification Report: \\n {classification_report(trainY, predictedY)}')\n",
    "\n",
    "# predict the Labels of the test set \n",
    "predictedY = np.rint(model.predict(testX))\n",
    "\n",
    "# print quality metrics \n",
    "print(f'Test Classification Report: \\n {classification_report(testY, predictedY)}')\n",
    "\n",
    "print('Train Confusion Matric: \\n')\n",
    "\n",
    "sn.heatmap(confusion_matrix(testY, predictedY), annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904445ac",
   "metadata": {},
   "source": [
    "I don't know what I'm doing wrong at this point. However, if I had this working, I would call the `LogisticClassifierGradient` 6 times in total, each time initializing with respect to a different gradient method, and selecting between explicit or estimated gradient. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74af171c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb112763",
   "metadata": {},
   "source": [
    "#### Logistic Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "9e82f622",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "\n",
    "from scipy.stats import multivariate_normal\n",
    "from scipy.special import expit\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import scale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecf408b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465a4790",
   "metadata": {},
   "source": [
    "6. [10 points] Write a multi-class version of the `BinaryLogisticClassifier` from lecture in the style of `scikit-learn` classifiers (i.e. as a Python class with `fit` and `predict` functions), optimized by gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75481568",
   "metadata": {},
   "source": [
    "Let's begin by redefining our `BinaryLogisticClassifier`, for which I will be using the equivalent `LogisticClassifierGradient` from the class github notebook `Week-3.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "fab4c75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryLogisticClassifier:\n",
    "        \n",
    "    # fit the model to the data\n",
    "    def fit(self, X, y, w0, alpha, h, tolerance, max_iterations):\n",
    "        \n",
    "        # save the training data\n",
    "        X = np.hstack((np.ones([X.shape[0], 1]), X))\n",
    "        \n",
    "        # standardize the data\n",
    "        X = scale(X)\n",
    "        \n",
    "        # find the w values that minimize the sum of squared errors via gradient descent\n",
    "        L = lambda w: (expit(X @ w).T - y.T) @ (expit(X @ w) - y)\n",
    "        self.w = self.gradientDescent(L, w0, alpha, h, tolerance, max_iterations)\n",
    "                \n",
    "    # predict the output from testing data\n",
    "    def predict(self, X):        \n",
    "        # append a column of ones at the beginning of X\n",
    "        X = np.hstack((np.ones([X.shape[0],1]), X))\n",
    "        \n",
    "        # standardize the data\n",
    "        X = scale(X)\n",
    "        \n",
    "        # return the predicted y's\n",
    "        return expit(X @ self.w)\n",
    "    \n",
    "    # run gradient descent to minimize the loss function\n",
    "    def gradientDescent(self, f, x0, alpha, h, tolerance, max_iterations):\n",
    "        # set x equal to the initial guess\n",
    "        x = x0\n",
    "\n",
    "        # take up to maxIterations number of steps\n",
    "        for counter in range(max_iterations):\n",
    "            # update the gradient\n",
    "            gradient = self.computeGradient(f, x, h)\n",
    "\n",
    "            # stop if the norm of the gradient is near 0\n",
    "            if np.linalg.norm(gradient) < tolerance:\n",
    "                print('Gradient descent took', counter, 'iterations to converge')\n",
    "                print('The norm of the gradient is', np.linalg.norm(gradient))\n",
    "                \n",
    "                # return the approximate critical value x\n",
    "                return x\n",
    "\n",
    "            # if we do not converge, print a message\n",
    "            elif counter == max_iterations - 1:\n",
    "                print(\"Gradient descent failed\")\n",
    "                print('The gradient is', gradient)\n",
    "                \n",
    "                # return x, sometimes it is still pretty good\n",
    "                return x\n",
    "\n",
    "            # take a step in the opposite direction as the gradient\n",
    "            x -= alpha*gradient\n",
    "            \n",
    "    # estimate the gradient\n",
    "    def computeGradient(self, f, x, h):\n",
    "        n = len(x)\n",
    "        gradient = np.zeros(n)\n",
    "        \n",
    "        # compute f at current point\n",
    "        fx = f(x)\n",
    "\n",
    "        # find each component of the gradient\n",
    "        for counter in range(n):\n",
    "            xUp = x.copy()\n",
    "            xUp[counter] += h\n",
    "            gradient[counter] = (f(xUp) - fx)/h\n",
    "\n",
    "        # return the gradient\n",
    "        return gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea03fd28",
   "metadata": {},
   "source": [
    "Next, we need to define a new multi-class verson, that will call upon this previous `BinaryLogisticClassifier` over multiple iterations. Within this class we need to create the following:\n",
    "1. A fit function that includes multiple classes instead of just one, and iterates over each class\n",
    "2. A predict function. that iterates over each classifier and selects the highest probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "49ba17bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiClassLogisticClassifier:\n",
    "   # fit the model to the data\n",
    "    def fit(self, X, y, w0, alpha, h, tolerance, max_iterations):\n",
    "        # declare list of binary classifiers\n",
    "        self.classifiers = []\n",
    "        \n",
    "        # find the number of unique classes\n",
    "        num_classes = len(np.unique(y))\n",
    "        \n",
    "        # iterate over each class for traning and fitting\n",
    "        for i in range (num_classes):\n",
    "            # create a binary label vector for the current class\n",
    "            label = LabelBinarizer(neg_label=0, pos_label=1)\n",
    "            y_binary = label.fit_transform(y)\n",
    "            y_binary = y_binary[:, i]\n",
    "            \n",
    "            # train and store binary classifier \n",
    "            classifier = BinaryLogisticClassifier()\n",
    "            classifier.fit(X, y_binary, w0, alpha, h, tolerance, max_iterations)\n",
    "            self.classifiers.append(classifier)\n",
    "    \n",
    "    # predict the output\n",
    "    def predict(self, X):\n",
    "        # declare array for class probabilities\n",
    "        probabilities = np.zeros((X.shape[0], len(self.classifiers)))\n",
    "        \n",
    "        # iterate over each classifier and update probabilities\n",
    "        for i in range(len(self.classifiers)):\n",
    "            probabilities[:,i] = self.classifiers[i].predict(X)\n",
    "            \n",
    "        # select the class with the highest probability \n",
    "        prediction = np.argmax(probabilities, axis=1)\n",
    "        \n",
    "        return prediction  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87a59eb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d370118",
   "metadata": {},
   "source": [
    "7. [10 points] Derive a formula for the gradient of the MSE loss function with respect to the weights of your multi-class logistic classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde0cc30",
   "metadata": {},
   "source": [
    "The MSE loss function can be expressed as follows:\n",
    "$MSE = \\frac{1}{n}\\sum_{n=1}^{n} (\\hat{Y}_i - Y_i)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401f4fe6",
   "metadata": {},
   "source": [
    "The gradient can be found by taking the derivative, which yields: $MSE = \\frac{1}{n}\\sum_{n=1}^{n} (\\hat{Y}_i - Y_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f92b9c",
   "metadata": {},
   "source": [
    "However, this only applies to a single logistic classifier, so for a multi-class classifier it is necessary to average over all the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebbb836",
   "metadata": {},
   "source": [
    "I don't know how to do this. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c19174",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914a5710",
   "metadata": {},
   "source": [
    "8. [10 points] Add functionality to optimize models using the three variants of gradient descent from Problems 2-4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de69caf",
   "metadata": {},
   "source": [
    "Add the other gradient descent variants as options, much like problem 5. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d0689a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee0c200",
   "metadata": {},
   "source": [
    "9. [15 points] Apply your classifier and all six variants of gradient descent (3 variants with estimated gradient and 3 variants with exact gradient) to classify the MNIST dataset. Compare the results in terms of test accuracy, `fit` runtime, and `predict` runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "5746b91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing MNIST\n",
    "mnist = datasets.load_digits()\n",
    "X = mnist.data.astype(float)\n",
    "y = mnist.target.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2880c1f7",
   "metadata": {},
   "source": [
    "Once the data is imported, run the classifiers much like problem 5. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
